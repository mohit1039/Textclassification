{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "textclassification.ipynb",
      "provenance": [],
      "toc_visible": true,
      "mount_file_id": "https://gist.github.com/mohit1039/b7e460cd67062c9b12b73be6bd6b7b7a#file-textclassification-ipynb",
      "authorship_tag": "ABX9TyMFdMQ6Vqr77lUU3LqGGQ5l",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mohit1039/Twitter-sentiment-analysis/blob/master/textclassification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gMhLXIdjEkj4",
        "colab_type": "code",
        "outputId": "a543c0a4-b686-43bd-b60b-7379d8d19881",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "cd /content/drive/My Drive/NLP"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X0xpV7_8FFHv",
        "colab_type": "text"
      },
      "source": [
        "#Dataset download"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p-s18--LElf4",
        "colab_type": "code",
        "outputId": "40b9a679-f1e0-4372-8251-84290f8ce8bd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "!wget http://www.cs.cornell.edu/people/pabo/movie-review-data/review_polarity.tar.gz"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-04-13 22:53:37--  http://www.cs.cornell.edu/people/pabo/movie-review-data/review_polarity.tar.gz\n",
            "Resolving www.cs.cornell.edu (www.cs.cornell.edu)... 132.236.207.20\n",
            "Connecting to www.cs.cornell.edu (www.cs.cornell.edu)|132.236.207.20|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 3127238 (3.0M) [application/x-gzip]\n",
            "Saving to: ‘review_polarity.tar.gz’\n",
            "\n",
            "review_polarity.tar 100%[===================>]   2.98M  1.50MB/s    in 2.0s    \n",
            "\n",
            "2020-04-13 22:53:40 (1.50 MB/s) - ‘review_polarity.tar.gz’ saved [3127238/3127238]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kvURcejOFVpi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!tar -xf review_polarity.tar.gz"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vBCH0eytFlb8",
        "colab_type": "text"
      },
      "source": [
        "#Import and load dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I0rmI4_mFp-J",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "e9ca645c-833c-46ca-aac5-ddcf8e3213bb"
      },
      "source": [
        "# Importing the libraries\n",
        "import numpy as np\n",
        "import re\n",
        "import pickle \n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from sklearn.datasets import load_files\n",
        "nltk.download('stopwords')"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZoBrYr8Dvphe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Importing the dataset\n",
        "reviews = load_files('txt_sentoken/')\n",
        "X,y = reviews.data,reviews.target"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nBjiQ0UeF6AL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Pickling the dataset\n",
        "with open('X.pickle','wb') as f:\n",
        "    pickle.dump(X,f)\n",
        "    \n",
        "with open('y.pickle','wb') as f:\n",
        "    pickle.dump(y,f)\n",
        "\n",
        "# Unpickling dataset\n",
        "X_in = open('X.pickle','rb')\n",
        "y_in = open('y.pickle','rb')\n",
        "X = pickle.load(X_in)\n",
        "y = pickle.load(y_in)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MDIEH9rRHMvv",
        "colab_type": "text"
      },
      "source": [
        "# preprocessing of dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_On_hAO8HR85",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "# Creating the corpus\n",
        "corpus = []\n",
        "for i in range(0, 2000):\n",
        "    #remove all punctuation marks i,e all non words character and subtitute space\n",
        "    review = re.sub(r'\\W', ' ', str(X[i]))\n",
        "    #convert review to lowercase\n",
        "    review = review.lower()\n",
        "    \n",
        "    review = re.sub(r'^br$', ' ', review)\n",
        "    \n",
        "    review = re.sub(r'\\s+br\\s+',' ',review)\n",
        "    #removing single character eg i,a\n",
        "    review = re.sub(r'\\s+[a-z]\\s+', ' ',review)\n",
        "    review = re.sub(r'^b\\s+', '', review)\n",
        "    #removing spaces\n",
        "    review = re.sub(r'\\s+', ' ', review)\n",
        "    corpus.append(review) "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MYP3dj8QJmBY",
        "colab_type": "text"
      },
      "source": [
        "# transforming in bag of words(BOW) model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V6dI7tA1JtJD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Creating the BOW model\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "vectorizer = CountVectorizer(max_features = 2000, min_df = 3, max_df = 0.6, stop_words = stopwords.words('english'))\n",
        "X = vectorizer.fit_transform(corpus).toarray()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xP0ObboUsyFk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Creating the Tf-Idf Model\n",
        "from sklearn.feature_extraction.text import TfidfTransformer\n",
        "transformer = TfidfTransformer()\n",
        "X = transformer.fit_transform(X).toarray()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WboZUP7Bs1G7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "# Creating the Tf-Idf model directly\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "vectorizer = TfidfVectorizer(max_features = 2000, min_df = 3, max_df = 0.6, stop_words = stopwords.words('english'))\n",
        "X = vectorizer.fit_transform(corpus).toarray()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4mTn-UZ8s6bl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Splitting the dataset into the Training set and Test set\n",
        "from sklearn.model_selection import train_test_split\n",
        "text_train, text_test, sent_train, sent_test = train_test_split(X, y, test_size = 0.20, random_state = 0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kUhp-4GBs9ht",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "d8988c23-d64e-484c-e5a9-d2e9081e5712"
      },
      "source": [
        "# Training the classifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "classifier = LogisticRegression()\n",
        "classifier.fit(text_train,sent_train)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
              "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
              "                   multi_class='auto', n_jobs=None, penalty='l2',\n",
              "                   random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
              "                   warm_start=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fS8KnztwtAzE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Testing model performance\n",
        "sent_pred = classifier.predict(text_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T0mdUTxgtDTe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "cm = confusion_matrix(sent_test, sent_pred)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-umixtbptHc7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Saving our classifier\n",
        "with open('classifier.pickle','wb') as f:\n",
        "    pickle.dump(classifier,f)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k-HdTiRxtJw-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Saving the Tf-Idf model\n",
        "with open('tfidfmodel.pickle','wb') as f:\n",
        "    pickle.dump(vectorizer,f)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wArYW0fttMeg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Using our classifier\n",
        "with open('tfidfmodel.pickle','rb') as f:\n",
        "    tfidf = pickle.load(f)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yEzWKRpqtPRj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with open('classifier.pickle','rb') as f:\n",
        "    clf = pickle.load(f)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9VzUDGdOtR9D",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "90da819d-892e-40c5-e14c-3448c161f9b5"
      },
      "source": [
        "sample = [\"You are a good person, have a great life\"]\n",
        "sample = tfidf.transform(sample).toarray()\n",
        "sentiment = clf.predict(sample)\n",
        "print(sentiment)"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1]\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}